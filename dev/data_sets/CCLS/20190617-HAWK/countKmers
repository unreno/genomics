#!/usr/bin/env bash


set -e	#	exit if any command fails
set -u	#	Error on usage of unset variables
set -o pipefail


CORES=40 #number of cores to use for blast searches
KMERSIZE=31 # RD:61


hawkDir=/home/jake/HAWK-0.9.8-beta
#	My version, 2.2.4 never completed?
jellyfishDir=${hawkDir}/supplements/jellyfish-Hawk/bin	# the included version is modified 1.1.6
#jellyfishDir=/usr/bin
#sortDir=/usr/bin # my normal version seems to be the parallel version


#for file in $( ls -d /raid/data/raw/CCLS/bam/*bam )
for file in $( ls /raid/data/raw/CCLS/bam/{GM_,}{983899,63185,268325,439338,634370}.recaled.bam )
do
	OUTPREFIX=$( basename $file	.recaled.bam )

	#if [ ${OUTPREFIX} == "120207" ] ; then
	#	echo "Skipping"
	#	continue
	#fi

	f=${OUTPREFIX}_kmers_jellyfish
	if [ -f $f ] && [ ! -w $f ] ; then
		echo "Write-protected $f exists. Skipping."
	else
		echo "Creating $f"

		mkdir ${OUTPREFIX}_kmers

#	I think that perhaps this samtools fastq should have some flags added to filter out only high quality, proper pair aligned reads?
#	Sadly "samtools fastq" does not have a -q quality filter as does "samtools view". Why not?
#	I suppose that I could pipe one to the other like ...
#		<( samtools view -h -q 40 -f 2 ${file} | samtools fastq - )

#	I think that when extracting reads from a bam, we probably shouldn't use the -C(canonical) flag, particularly when select high quality mappings

		date
		#${jellyfishDir}/jellyfish count --canonical --output ${OUTPREFIX}_kmers/tmp --mer-len ${KMERSIZE} --threads ${CORES} --size 5G \
		#	<( samtools view -h -q 40 -f 2 ${file} | samtools fastq - )
		${jellyfishDir}/jellyfish count --output ${OUTPREFIX}_kmers/tmp --mer-len ${KMERSIZE} --threads ${CORES} --size 5G \
			<( samtools view -h -q 40 -f 2 ${file} | samtools fastq - )
		date


		COUNT=$(ls ${OUTPREFIX}_kmers/tmp* |wc -l)

		if [ $COUNT -eq 1 ]
		then
 			mv ${OUTPREFIX}_kmers/tmp_0 ${f}
		else
			${jellyfishDir}/jellyfish merge -o ${f} ${OUTPREFIX}_kmers/tmp*
		fi
		rm -rf ${OUTPREFIX}_kmers

		chmod a-w $f
	fi

	f=${OUTPREFIX}.kmers.hist.csv
	if [ -f $f ] && [ ! -w $f ] ; then
		echo "Write-protected $f exists. Skipping."
	else
		echo "Creating $f"

		f2=${OUTPREFIX}.kmers.jellyfish.hist.csv
		if [ -f $f2 ] && [ ! -w $f2 ] ; then
			echo "Write-protected $f2 exists. Skipping."
		else
			echo "Creating $f2"
			${jellyfishDir}/jellyfish histo --full --output ${f2} --threads ${CORES} ${OUTPREFIX}_kmers_jellyfish
			chmod a-w $f2
		fi

		#	swap, for some reason
		awk '{print $2"\t"$1}' ${OUTPREFIX}.kmers.jellyfish.hist.csv > ${f}
		chmod a-w $f

		rm -rf $f2
	fi


	f=${OUTPREFIX}_total_kmer_counts.txt
	if [ -f $f ] && [ ! -w $f ] ; then
		echo "Write-protected $f exists. Skipping."
	else
		echo "Creating $f"
		awk -f ${hawkDir}/countTotalKmer.awk ${OUTPREFIX}.kmers.hist.csv > ${f}
#		awk -f ${hawkDir}/countTotalKmer.awk ${OUTPREFIX}.kmers.hist.csv >> total_kmer_counts.txt
		chmod a-w $f
	fi

	f=${OUTPREFIX}_kmers_sorted.txt
	if [ -f $f ] && [ ! -w $f ] ; then
		echo "Write-protected $f exists. Skipping."
	else

		#	Original version does this with CUTOFF. I have no idea why.
		#	Set it to 1, output it to a file, then add 1 and use as the lower limit. Everytime? Why not just fixed as 2?
		#	CUTOFF=1 
		#	echo $CUTOFF > ${OUTPREFIX}_cutoff.csv
		#	${jellyfishDir}/jellyfish dump -c -L `expr $CUTOFF + 1` ${OUTPREFIX}_kmers_jellyfish > ${OUTPREFIX}_kmers.txt 

		f2=${OUTPREFIX}_kmers.txt
		if [ -f $f2 ] && [ ! -w $f2 ] ; then
			echo "Write-protected $f2 exists. Skipping."
		else
			echo "Creating $f2"
			${jellyfishDir}/jellyfish dump --column --lower-count 2 ${OUTPREFIX}_kmers_jellyfish > ${f2}
			chmod a-w $f2
		fi

		echo "Creating $f"
		sort --parallel=${CORES} -n -k 1 ${OUTPREFIX}_kmers.txt > ${f}
		chmod a-w $f

		rm -rf $f2
	fi

#	rm ${OUTPREFIX}_kmers_jellyfish
#	rm ${OUTPREFIX}_kmers.txt

#	echo "${OUTPREFIX}_kmers_sorted.txt" >> sorted_files.txt

done
