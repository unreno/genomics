

For purposes of clustering, starting bulk instances, never manually connecting, having them complete processing and then shut themselves down and go away, the following notes apply.

First off, the instance(s) should be started with the "terminate" behaviour.
Otherwise, the stay on your dashboard. You may actually be able to restart it.

aws ec2 run-instances --instance-initiated-shutdown-behavior terminate


Also, the instance must allow the scripting of sudo and shutting down without being logged in.

sudo shutdown -h now


#	sudo raises this error
#	sudo: sorry, you must have a tty to run sudo
#	adding -t, -tt, -ttt, -tttt to ssh doesn't change this but can get this instead....
#	Pseudo-terminal will not be allocated because stdin is not a terminal.
#	Only ...
#
# sudo visudo 
#   to comment out the following lines ...
# Defaults    requiretty
# Defaults   !visiblepw
#
#	works.  Need to create yet another AMI with this change.
#	http://unix.stackexchange.com/questions/49077

You'll need to make those mods and create your own AMI.
I don't believe that this is scriptable on instance creation of a standard instance.


As AWS charges by the hour, use with caution.
If you start 100 instances and they immediately commit suicide,
that just cost you $5, or more depending on your options.




For 1000 genomes processing, I added the following option
when starting the instances.

--user-data file://aws_start_1000genomes_processing.sh

This loaded a small LOCAL script as data and passed it to the new instance as ROOT.
I changed to the basic user for my script, which on completion called "sudo shutdown -h now"

#!/usr/bin/env bash
su -l ec2-user -c 'nohup aws_1000genomes.sh --shutdown &> ~/aws_1000genomes.sh.log &'


